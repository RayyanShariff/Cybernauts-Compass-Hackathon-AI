{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, pipeline, set_seed\n",
    "from gtts import gTTS\n",
    "import os\n",
    "\n",
    "# Running server\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import nbformat\n",
    "from nbconvert.preprocessors import ExecutePreprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for creating and refining prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\moham\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "': I like coding!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the T5 model and tokenizer\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "def refine_prompt(user_input):\n",
    "    # Encode the input text\n",
    "    input_ids = t5_tokenizer.encode(f\"refine: {user_input}\", return_tensors=\"pt\")\n",
    "    # Generate refined prompt\n",
    "    outputs = t5_model.generate(input_ids)\n",
    "    # Decode the output text\n",
    "    refined_prompt = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return refined_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'refine: Does the model even change this text?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refine_prompt(\"Does the model even change this text?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for Generating Textual Output, Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substring_until_last_full_stop(text):\n",
    "    last_full_stop_index = text.rfind('.')\n",
    "    if last_full_stop_index != -1:\n",
    "        return text[:last_full_stop_index + 1]\n",
    "    return text  # Return the original text if no full stop is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [08/Jun/2024 03:47:45] \"OPTIONS /generate HTTP/1.1\" 200 -\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the scenario:\n",
      "Generate a detailed description for the following scenario: Horses on a rack\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [08/Jun/2024 03:48:01] \"POST /generate HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the generated text from the model:\n",
      "Generate a detailed description for the following scenario: Horses on a rack:\n",
      "\n",
      "(Note that the horses are also not in any way being chased in the scene, i.e. no horses should be attacked). Also note:\n",
      "\n",
      "The horses are not allowed to fly above the wall in any way. The same rule as in original scenario: no vehicles.\n",
      "\n",
      "The horses may then run over each other up to the tower with both men and women.\n",
      "\n",
      "The horses will not enter any of the following areas:\n",
      "\n",
      "\n",
      "This can be avoided by attacking the entrances of the structures, or they are being blocked.\n",
      "\n",
      "There may be no horses in any order, but with at least one (maybe two) to two (\n",
      "This is the text after substringing it:\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "# Initialize the GPT-2 pipeline\n",
    "set_seed(42)\n",
    "text_generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return \"Flask server is running!\"\n",
    "\n",
    "@app.route('/generate', methods=['POST'])\n",
    "def generate():\n",
    "    data = request.get_json()\n",
    "    user_input=''\n",
    "    user_input = data.get('user_input', '')\n",
    "\n",
    "    # Create a prompt for GPT-2\n",
    "    prompt = f\"Generate a detailed description for the following scenario: {user_input}\"\n",
    "    print(\"This is the scenario:\")\n",
    "    print(prompt)\n",
    "\n",
    "    # Generate text\n",
    "    generated_text = text_generator(str(prompt), max_length=150, num_return_sequences=1)[0]['generated_text']\n",
    "\n",
    "    print(\"This is the generated text from the model:\")\n",
    "    print(generated_text)\n",
    "\n",
    "    print(\"This is the text after substringing it:\")\n",
    "    generated_text = substring_until_last_full_stop(generated_text)\n",
    "\n",
    "    # Here, you can integrate video generation logic using generated_text\n",
    "    # For now, we return the generated text as a placeholder\n",
    "    return jsonify({'generated_text': generated_text})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
